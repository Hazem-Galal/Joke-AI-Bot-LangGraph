{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9cf6bd8",
   "metadata": {},
   "source": [
    "# ü§ñ Joke AI Bot with LangGraph\n",
    "\n",
    "An interactive joke bot built with **LangGraph** and **OpenAI GPT-4o**.\n",
    "\n",
    "The bot follows a stateful graph workflow:\n",
    "- **Greet User** ‚Üí Welcome the user and ask if they want a joke\n",
    "- **Get & Check Topic** ‚Üí Get a topic from the user and validate it (loops back if invalid)\n",
    "- **Continue?** ‚Üí Ask the user if they want another joke\n",
    "- **Say Bye** ‚Üí Farewell message with joke count\n",
    "\n",
    "![Architecture](https://raw.githubusercontent.com/Hazem-Galal/Joke-AI-Bot-LangGraph/main/joke%20bot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f855e6f",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a4e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langgraph langchain-openai langchain-core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7695bc38",
   "metadata": {},
   "source": [
    "## 2. Setup API Key\n",
    "\n",
    "Add your OpenAI API key to **Colab Secrets**:\n",
    "1. Click the üîë icon in the left sidebar\n",
    "2. Add a new secret named `OPENAI_API_KEY`\n",
    "3. Paste your OpenAI API key as the value\n",
    "4. Toggle \"Notebook access\" ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e819bf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
    "print(\"‚úÖ API key loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f43633",
   "metadata": {},
   "source": [
    "## 3. Define the State\n",
    "\n",
    "The state holds all shared data across nodes in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a68015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class JokeBotState(TypedDict):\n",
    "    \"\"\"State schema for the Joke Bot.\"\"\"\n",
    "    messages: Annotated[list[AnyMessage], add_messages]  # Conversation history\n",
    "    topic: str              # Current joke topic\n",
    "    is_valid_topic: bool    # Whether the topic is valid for jokes\n",
    "    user_choice: str        # User's choice: 'continue' or 'end'\n",
    "    joke_count: int         # Number of jokes told\n",
    "\n",
    "\n",
    "print(\"‚úÖ State defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3047084",
   "metadata": {},
   "source": [
    "## 4. Initialize the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5ccd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.9)\n",
    "\n",
    "print(\"‚úÖ LLM initialized (GPT-4o)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454fe2ef",
   "metadata": {},
   "source": [
    "## 5. Define Node Functions\n",
    "\n",
    "Each node is a Python function that receives the current state, performs logic, and returns state updates.\n",
    "\n",
    "| Node | Description |\n",
    "|------|-------------|\n",
    "| **Greet User** | Welcomes the user and asks if they want a joke |\n",
    "| **Get & Check Topic** | Prompts for a topic, validates it, and generates a joke |\n",
    "| **Continue?** | Asks if the user wants another joke |\n",
    "| **Say Bye** | Farewell message with total joke count |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7764c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Node 1: Greet User\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def greet_user(state: JokeBotState) -> dict:\n",
    "    \"\"\"Welcome the user and ask if they want to hear a joke.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"üé≠  Welcome to the Joke AI Bot!\")\n",
    "    print(\"    Powered by LangGraph & GPT-4o\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\nI can tell you jokes on any topic you like!\")\n",
    "\n",
    "    choice = input(\"\\nWould you like to hear a joke? (yes/no): \").strip().lower()\n",
    "    user_choice = \"continue\" if choice in [\"yes\", \"y\", \"sure\", \"ok\", \"yeah\"] else \"end\"\n",
    "\n",
    "    return {\n",
    "        \"user_choice\": user_choice,\n",
    "        \"messages\": [AIMessage(content=\"Welcome to the Joke AI Bot! üé≠\")],\n",
    "    }\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Node 2: Get & Check Topic\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def get_and_check_topic(state: JokeBotState) -> dict:\n",
    "    \"\"\"Get a topic from the user, validate it, and generate a joke if valid.\"\"\"\n",
    "    topic = input(\"\\nüéØ Enter a topic for your joke: \").strip()\n",
    "\n",
    "    if not topic:\n",
    "        print(\"‚ùå You didn't enter a topic. Please try again.\")\n",
    "        return {\n",
    "            \"topic\": \"\",\n",
    "            \"is_valid_topic\": False,\n",
    "        }\n",
    "\n",
    "    # Ask the LLM to validate the topic and generate a joke\n",
    "    validation_prompt = [\n",
    "        SystemMessage(content=(\n",
    "            \"You are a helpful joke validation and generation assistant. \"\n",
    "            \"The user will provide a topic. First, determine if the topic is appropriate \"\n",
    "            \"and suitable for a joke (not offensive, not nonsensical gibberish). \"\n",
    "            \"If the topic is VALID: respond with 'VALID:' followed by a short, clever, \"\n",
    "            \"and funny joke about that topic. \"\n",
    "            \"If the topic is INVALID: respond with 'INVALID:' followed by a brief \"\n",
    "            \"explanation of why it's not suitable.\"\n",
    "        )),\n",
    "        HumanMessage(content=f\"Topic: {topic}\"),\n",
    "    ]\n",
    "\n",
    "    response = llm.invoke(validation_prompt)\n",
    "    response_text = response.content\n",
    "\n",
    "    if response_text.upper().startswith(\"VALID:\"):\n",
    "        joke = response_text[6:].strip()\n",
    "        print(f\"\\nüòÇ Here's your joke about '{topic}':\\n\")\n",
    "        print(f\"   {joke}\")\n",
    "        print()\n",
    "        return {\n",
    "            \"topic\": topic,\n",
    "            \"is_valid_topic\": True,\n",
    "            \"joke_count\": state.get(\"joke_count\", 0) + 1,\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=f\"Tell me a joke about: {topic}\"),\n",
    "                AIMessage(content=joke),\n",
    "            ],\n",
    "        }\n",
    "    else:\n",
    "        reason = response_text[8:].strip() if response_text.upper().startswith(\"INVALID:\") else response_text\n",
    "        print(f\"\\n‚ùå That topic isn't great for a joke: {reason}\")\n",
    "        print(\"   Please try a different topic.\\n\")\n",
    "        return {\n",
    "            \"topic\": topic,\n",
    "            \"is_valid_topic\": False,\n",
    "        }\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Node 3: Continue?\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def continue_prompt(state: JokeBotState) -> dict:\n",
    "    \"\"\"Ask the user if they want another joke.\"\"\"\n",
    "    joke_count = state.get(\"joke_count\", 0)\n",
    "    print(f\"üìä Jokes told so far: {joke_count}\")\n",
    "\n",
    "    choice = input(\"\\nWould you like another joke? (yes/no): \").strip().lower()\n",
    "    user_choice = \"continue\" if choice in [\"yes\", \"y\", \"sure\", \"ok\", \"yeah\", \"more\"] else \"end\"\n",
    "\n",
    "    return {\"user_choice\": user_choice}\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Node 4: Say Bye\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def say_bye(state: JokeBotState) -> dict:\n",
    "    \"\"\"Say goodbye to the user with a summary.\"\"\"\n",
    "    joke_count = state.get(\"joke_count\", 0)\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"üëã Thanks for using the Joke AI Bot!\")\n",
    "    print(f\"   I told you {joke_count} joke(s) today.\")\n",
    "    if joke_count > 0:\n",
    "        print(\"   Hope I made you laugh! üòÑ\")\n",
    "    else:\n",
    "        print(\"   Maybe next time! üòä\")\n",
    "    print(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"Goodbye! I told you {joke_count} joke(s). See you next time! üëã\")],\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úÖ All node functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c50957",
   "metadata": {},
   "source": [
    "## 6. Define Routing Functions\n",
    "\n",
    "Routing functions determine which node to execute next based on the current state.\n",
    "\n",
    "| After Node | Condition | Next Node |\n",
    "|---|---|---|\n",
    "| Greet User | `continue` | Get & Check Topic |\n",
    "| Greet User | `end` | Say Bye |\n",
    "| Get & Check Topic | Valid topic | Continue? |\n",
    "| Get & Check Topic | Invalid topic | Get & Check Topic (self-loop) |\n",
    "| Get & Check Topic | `end` | Say Bye |\n",
    "| Continue? | `continue` | Get & Check Topic |\n",
    "| Continue? | `end` | Say Bye |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cab945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_after_greet(state: JokeBotState) -> str:\n",
    "    \"\"\"Route after greeting: continue to get topic or say bye.\"\"\"\n",
    "    if state.get(\"user_choice\") == \"continue\":\n",
    "        return \"get_and_check_topic\"\n",
    "    return \"say_bye\"\n",
    "\n",
    "\n",
    "def route_after_topic(state: JokeBotState) -> str:\n",
    "    \"\"\"Route after topic check: loop back if invalid, continue if valid, or end.\"\"\"\n",
    "    if state.get(\"user_choice\") == \"end\":\n",
    "        return \"say_bye\"\n",
    "    if not state.get(\"is_valid_topic\", False):\n",
    "        return \"get_and_check_topic\"  # Self-loop: invalid topic\n",
    "    return \"continue_prompt\"  # Valid topic ‚Üí ask to continue\n",
    "\n",
    "\n",
    "def route_after_continue(state: JokeBotState) -> str:\n",
    "    \"\"\"Route after continue prompt: get another topic or say bye.\"\"\"\n",
    "    if state.get(\"user_choice\") == \"continue\":\n",
    "        return \"get_and_check_topic\"\n",
    "    return \"say_bye\"\n",
    "\n",
    "\n",
    "print(\"‚úÖ Routing functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532c8940",
   "metadata": {},
   "source": [
    "## 7. Build & Compile the Graph\n",
    "\n",
    "Connect all nodes and edges to form the complete workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bb160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Create the graph\n",
    "workflow = StateGraph(JokeBotState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"greet_user\", greet_user)\n",
    "workflow.add_node(\"get_and_check_topic\", get_and_check_topic)\n",
    "workflow.add_node(\"continue_prompt\", continue_prompt)\n",
    "workflow.add_node(\"say_bye\", say_bye)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"greet_user\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"greet_user\",\n",
    "    route_after_greet,\n",
    "    {\n",
    "        \"get_and_check_topic\": \"get_and_check_topic\",\n",
    "        \"say_bye\": \"say_bye\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"get_and_check_topic\",\n",
    "    route_after_topic,\n",
    "    {\n",
    "        \"get_and_check_topic\": \"get_and_check_topic\",  # Self-loop (invalid topic)\n",
    "        \"continue_prompt\": \"continue_prompt\",           # Valid topic\n",
    "        \"say_bye\": \"say_bye\",                           # End\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"continue_prompt\",\n",
    "    route_after_continue,\n",
    "    {\n",
    "        \"get_and_check_topic\": \"get_and_check_topic\",  # Another joke\n",
    "        \"say_bye\": \"say_bye\",                           # End\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"say_bye\", END)\n",
    "\n",
    "# Compile the graph\n",
    "graph = workflow.compile()\n",
    "\n",
    "print(\"‚úÖ Graph built and compiled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284e5b12",
   "metadata": {},
   "source": [
    "## 8. Visualize the Graph\n",
    "\n",
    "Render the graph to verify it matches the intended architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70765c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not render graph image: {e}\")\n",
    "    print(\"\\nGraph structure (Mermaid format):\")\n",
    "    print(graph.get_graph().draw_mermaid())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd650d96",
   "metadata": {},
   "source": [
    "## 9. Run the Bot üöÄ\n",
    "\n",
    "Start the interactive joke bot! Follow the prompts to get jokes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9e8a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial state\n",
    "initial_state = {\n",
    "    \"messages\": [],\n",
    "    \"topic\": \"\",\n",
    "    \"is_valid_topic\": False,\n",
    "    \"user_choice\": \"\",\n",
    "    \"joke_count\": 0,\n",
    "}\n",
    "\n",
    "# Run the graph\n",
    "final_state = graph.invoke(initial_state, config={\"recursion_limit\": 100})\n",
    "\n",
    "print(\"\\nüìú Conversation Summary:\")\n",
    "print(\"-\" * 40)\n",
    "for msg in final_state[\"messages\"]:\n",
    "    role = \"ü§ñ Bot\" if isinstance(msg, AIMessage) else \"üë§ You\"\n",
    "    print(f\"{role}: {msg.content}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total jokes: {final_state['joke_count']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
